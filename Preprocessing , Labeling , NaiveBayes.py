# -*- coding: utf-8 -*-
"""Another copy of Bab3,4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fgicX8F8auEq78BPNHsMw2Yx5YZuEI2E
"""

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

# Set option for displaying maximum columns
pd.set_option('display.max_columns', None)

# Read the CSV file
df = pd.read_csv('/content/drive/MyDrive/Prepocessingnewtestt.csv', sep=';')

# Display the first few rows of the DataFrame
df.head()

df = df[['full_text']]
df

"""##CLEANING"""

df.shape

df = df.drop_duplicates(subset=['full_text'])
df.duplicated().sum()
df = df.dropna()
df.isnull().sum()
df.shape

def remove_URL(tweet):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'',tweet)
def remove_html(tweet):
    html = re.compile(r'<.*?>')
    return html.sub(r'',tweet)
def remove_numbers(tweet):
    tweet = re.sub(r'\d+','',tweet)
    return tweet
def remove_symbols(tweet):
    tweet = re.sub(r'[^a-zA-Z0-9\s]','',tweet) #MENGHAPUS SEMUA SIMBOL
    return tweet
def remove_username(tweet):
    tweet = re.sub('@[^\s]+','',tweet)
    return tweet

df['Cleaning']= df['full_text'].apply(lambda x:remove_URL(x))
df['Cleaning']= df['full_text'].apply(lambda x:remove_html(x))
df['Cleaning']= df['full_text'].apply(lambda x:remove_numbers(x))
df['Cleaning']= df['full_text'].apply(lambda x:remove_symbols(x))
df['Cleaning']= df['full_text'].apply(lambda x:remove_username(x))

df.head(100)

"""## CASE FOLDING

"""

def case_folding(text):
    if isinstance(text, str):
      lowercase_text = text.lower()
      return lowercase_text

    else:
      return text

df['Case_folding'] = df['Cleaning'].apply(case_folding)
df.head(100)

"""## NORMALIZATION"""

# NORMALIZATION
norm = {"org" : "orang","tp" : "tetapi" , "tdk" : "tidak", "utk" : "untuk" , "bgt" : "banget", "mkn" : "makan" , "krn" : "karena", "yg" : "yang" , "skrng" : "sekarang", "kaya" : "seperti" , "trus" : "terus", "ga" : "tidak", "jgn":"jangan","tp":"tetapi","bkn":"bukan"
, "blm" : "belum"}
def normalisasi(str_text):
  for i in norm:
    str_text = str_text.replace(i, norm[i])
  return str_text

df['Normalization'] = df['Case_folding'].apply(lambda x: normalisasi(x))
df.head(100)

"""## STOPWORDS"""

!pip install Sastrawi

import Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
more_stop_words = ["yang", "yg"]

stop_words = StopWordRemoverFactory().get_stop_words()
stop_words.extend(more_stop_words)

new_array = ArrayDictionary(stop_words)
stop_words_remover_new = StopWordRemover(new_array)

def stopword(str_text):
  str_text = stop_words_remover_new.remove(str_text)
  return str_text

df['full_text'] = df['Normalization'].apply(lambda x: stopword(x))
df.head()

"""## TOKENIZE"""

tokenized = df['Normalization'].apply(lambda x: x.split())

# Menampilkan setiap list dalam tokenized
for tokens in tokenized:
    print(tokens)

"""## STEMMING"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(text_cleaning):
  factory = StemmerFactory()
  stemmer = factory.create_stemmer()
  do = []

  for w in text_cleaning:
    dt = stemmer.stem(w)
    do.append(dt)
  d_clean = []
  d_clean = " ".join(do)
  print(d_clean)
  return d_clean

tokenized = tokenized.apply(stemming)

tokenized.to_csv("tesnew.csv", index=False)

"""## TRANSLATE"""

!pip install translate

data = pd.read_csv('/content/drive/MyDrive/cobabaru.csv', encoding='latin1', delimiter=';')
data.head()

from translate import Translator

def convert_eng(tweet):
  translator = Translator(to_lang="en", from_lang="en")
  translation = translator.translate(tweet)
  return translation

data['tweet_english'] = data['full_text'].apply(convert_eng)
data.to_csv("cobane.csv")

"""## LABELING"""

import pandas as pd

# Read the CSV file
data = pd.read_csv("/content/drive/MyDrive/cobane.csv", index_col=0, encoding='latin1', sep=",")
data

!pip install tweet-preprocessor
!pip install textblob
!pip install wordcloud
!pip install nltk

import preprocessor as p
from textblob import TextBlob
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

nltk.download('punkt')

data_tweet = list(data['tweet_english'])
polaritas = 0

status = []
total_positif = total_negatif = total_netral = total = 0

for i, tweet in enumerate(data_tweet):
  analysis = TextBlob(tweet)
  polaritas += analysis.polarity

  if analysis.sentiment.polarity < 0:
    total_negatif += 1
    status.append('Negatif')
  elif analysis.sentiment.polarity == 0:
    total_netral += 1
    status.append('Netral')
  else:
    total_positif += 1
    status.append('Positif')

  total += 1

print(f'Hasil Analisis Data:\nPositif = {total_positif}\nNetral = {total_netral}\nNegatif = {total_negatif}')
print(f'\nTotal Data : {total}')

"""## VISUALISASI"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

def plot_cloud(wordcloud):
    plt.figure(figsize=(10, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

# Pastikan untuk mengonversi setiap elemen menjadi string menggunakan str()
all_words = ' '.join([str(tweets) for tweets in data['tweet_english']])

wordcloud = WordCloud(
    width=3000,
    height=2000,
    random_state=3,
    background_color='black',
    colormap='RdPu',
    collocations=False,
    stopwords=STOPWORDS
).generate(all_words)

plot_cloud(wordcloud)

data['klasifikasi'] = status
data

import matplotlib.pyplot as plt

total_positif = 687
total_negatif = 234
total_netral = 87

def show_pie_chart(labels, counts, title):
    fig, ax = plt.subplots(figsize=(8, 6))
    wedges, texts, autotexts = ax.pie(counts, labels=labels, autopct='%1.0f%%', startangle=60, colors=['#008000', '#800000', '#191970'])
    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    ax.set_title(title)

    # Menambahkan anotasi jumlah tweet di dalam lingkaran
    for i, (text, autotext) in enumerate(zip(texts, autotexts)):
        autotext.set_color('Snow')
        autotext.set_fontweight('bold')
        text.set_text(f'{labels[i]}\n({counts[i]} tweets)')

    plt.show()

labels = ['Positif', 'Negatif', 'Netral']
counts = [total_positif, total_negatif, total_netral]

show_pie_chart(labels, counts, "Sentimen Program Makan Siang Gratis")

"""## KLASIFIKASI NAIVE BAYES"""

data

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score
import seaborn as sns

"""## SPLIT DATASET DAN DATA UJI"""

X_train, X_test, y_train, y_test = train_test_split(data['tweet_english'], data['klasifikasi'], test_size=0.2, random_state=42)

"""## Mencetak Jumlah Data"""

print(f'Jumlah Data Latih: {len(X_train)}')
print(f'Jumlah Data Uji: {len(X_test)}')

"""## Mengubah teks menjadi vektor"""

vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

print('Hasil Ekstraksi:')
print('---------------')
print('Vektor fitur Data Latih:')
print(X_train_vectorized.toarray())
print('\nVektor Fitur Data Uji:')
print(X_test_vectorized.toarray())

"""## TRAIN MODEL"""

model = MultinomialNB()
model.fit(X_train_vectorized, y_train)

"""## EVALUASI MODEL"""

predictions = model.predict(X_test_vectorized)
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions, average='weighted')
recall = recall_score(y_test, predictions, average='weighted')
f1 = f1_score(y_test, predictions, average='weighted')

print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'Accuracy: {accuracy:2f}')
print(f'F1-score: {f1:.2f}')
print('\nHasil Klasifikasi: \n', classification_report(y_test, predictions))
print('\nConfussion Matrix:\n', confusion_matrix(y_test, predictions))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_test, predictions)

# PLOT MATRIX
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=['Negatif','Netral','Positif'], yticklabels=['Negatif', 'Netral', 'Positif'])
plt.title('Confusion Matrix')
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.show()

"""## EVALUASI MODEL MENGGUNAKAN DATA LATIH"""

# Evaluasi Model
predictions_train = model.predict(X_train_vectorized)

# Akurasi
accuracy_train = accuracy_score(y_train, predictions_train)
print(f'Accuracy on Training Data: {accuracy_train:.2f}')

# Hasil Klasifikasi
print('\nHasil Klasifikasi dengan data train:\n', classification_report(y_train, predictions_train))

conf_matrix_train = confusion_matrix(y_train, predictions_train)

# PLOT Confusion Matrix untuk data latih
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='RdPu', xticklabels=['Negatif','Netral','Positif'], yticklabels=['Negatif', 'Netral', 'Positif'])
plt.title('Confusion Matrix on Data Train')
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.show()

# Spasi antara gambar
plt.subplots_adjust(wspace=0.5)

total_positif = 687
total_negatif = 234
total_netral = 87

def show_pie_chart(labels, counts, title):
    fig, ax = plt.subplots(figsize=(6, 6))
    wedges, texts, autotexts = ax.pie(counts, labels=labels, autopct='%1.0f%%', startangle=60, colors=['#008000', '#800000', '#191970'])
    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    ax.set_title(title)

    # Menambahkan anotasi jumlah tweet di dalam lingkaran
    for i, (text, autotext) in enumerate(zip(texts, autotexts)):
        autotext.set_color('Snow')
        autotext.set_fontweight('bold')
        text.set_text(f'{labels[i]}\n({counts[i]} tweets)')

    plt.show()

labels = ['Positif', 'Negatif', 'Netral']
counts = [total_positif, total_negatif, total_netral]

show_pie_chart(labels, counts, "Sentimen Program Makan Siang Gratis")